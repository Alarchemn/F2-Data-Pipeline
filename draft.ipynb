{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74910bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import requests\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from io import StringIO\n",
    "import os\n",
    "import html5lib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377f2888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas==1.5.3\n",
      "bs4==4.12.2\n",
      "requests==2.29.0\n",
      "html5lib==1.1\n"
     ]
    }
   ],
   "source": [
    "print(f'pandas=={pd.__version__}')\n",
    "print(f'bs4=={bs4.__version__}')\n",
    "print(f'requests=={requests.__version__}')\n",
    "print(f'html5lib=={html5lib.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ead8a",
   "metadata": {},
   "source": [
    "# 1- EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Race id's per season\n",
    "season_2017_id = ['972','974','975','976','977','979','978','980','981','982','973']\n",
    "season_2018_id = ['983','984','984','985','986','987','988','989','990','991','992','993','994']\n",
    "season_2019_id = ['1007','996','997','1008','999','1000','1001','1002','1003','1004','1005','1006']\n",
    "season_2020_id = ['1012','1021','1015','1014','1022','1010','1016','1017','1023','1024','1025','1026']\n",
    "season_2021_id = ['1027','1028','1029','1030','1031','1032','1033','1034']\n",
    "season_2022_id = ['1035','1036','1037','1038','1039','1040','1041','1042','1049','1043','1044','1045','1046','1048']\n",
    "season_2023_id = ['1050','1051','1052','1053','1055','1056']\n",
    "\n",
    "all_race_ids = []\n",
    "all_race_ids.extend(season_2017_id)\n",
    "all_race_ids.extend(season_2018_id)\n",
    "all_race_ids.extend(season_2019_id)\n",
    "all_race_ids.extend(season_2020_id)\n",
    "all_race_ids.extend(season_2021_id)\n",
    "all_race_ids.extend(season_2022_id)\n",
    "all_race_ids.extend(season_2023_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fde18",
   "metadata": {},
   "source": [
    "## 1.1- Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1242bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(raceid):\n",
    "    \"\"\"\n",
    "    Extracts data from the FIA Formula 2 website for the given race IDs.\n",
    "    \n",
    "    Args:\n",
    "        raceid (str): str with the race IDs. unfortunately they do not have a clear order\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Extracted data as a DataFrame.\n",
    "        whit data from Race, Sprint Race/Practice2, Practice1 and Qualy of the event\n",
    "    \"\"\" \n",
    "\n",
    "    ENDPOINT = 'http://www.fiaformula2.com/Results?raceid='\n",
    "    # Create an empty DataFrame to store the extracted data\n",
    "    df = pd.DataFrame()  \n",
    "    # Request the data\n",
    "    url = requests.get(ENDPOINT + raceid)\n",
    "    # Create a BeautifulSoup object  \n",
    "    soup = BeautifulSoup(url.text, 'html.parser')    \n",
    "    # Read HTML tables from the web page\n",
    "    raw = pd.read_html(url.text)  \n",
    "    \n",
    "    circuit = soup.find(class_='country-circuit').text  # Extract circuit information\n",
    "    schedule = soup.find(class_='schedule').text  # Extract schedule information\n",
    "    tables = soup.find_all('table')  # Find all tables on the web page (4 tables)\n",
    "    events = soup.find_all(class_='collapsible-header')  # Find all collapsible headers (Event information)\n",
    "    \n",
    "    for index_out in range(len(tables)):\n",
    "        pos = tables[index_out].find_all('div', class_='pos')  # Find all pilot race positions in the table\n",
    "        car_no = tables[index_out].find_all('div', class_='car-no')  # Find car numbers in the table\n",
    "        names = tables[index_out].find_all('div', class_='driver-name')  # Find driver names in the table\n",
    "\n",
    "        event = events[index_out].find('span').text  # Extract event information\n",
    "        \n",
    "        # Create empty lists to store extracted data for each row\n",
    "        pos_data = []\n",
    "        car_no_data = []\n",
    "        driver_name_data = []\n",
    "        team_name_data = []\n",
    "        circuit_data = []\n",
    "        schedule_data = []\n",
    "        event_data = []\n",
    "\n",
    "        for index_in in range(len(pos)):\n",
    "            # Append extracted data to the respective lists\n",
    "            pos_data.append(pos[index_in].text)\n",
    "            car_no_data.append(car_no[index_in].text)\n",
    "            driver_name_data.append(names[index_in].find_all(class_='visible-desktop-up')[0].text)\n",
    "            team_name_data.append(names[index_in].find_all(class_='team-name')[0].text)\n",
    "            circuit_data.append(circuit)\n",
    "            schedule_data.append(schedule)\n",
    "            event_data.append(event)\n",
    "\n",
    "        raw[index_out].drop('POSNumber / Driver and TeamNo / Driver', axis=1, inplace=True)\n",
    "        raw[index_out]['POS'] = pos_data\n",
    "        raw[index_out]['CAR'] = car_no_data\n",
    "        raw[index_out]['PILOT NAME'] = driver_name_data\n",
    "        raw[index_out]['TEAM'] = team_name_data\n",
    "        raw[index_out]['CIRCUIT'] = circuit_data\n",
    "        raw[index_out]['SCHEDULE'] = schedule_data\n",
    "        raw[index_out]['TYPE'] = event_data\n",
    "\n",
    "        # Concatenate each table (Race, Sprint Race/Practice2, Practice1 and Qualy) data to the full DataFrame\n",
    "        df = pd.concat([df, raw[index_out]], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf072c",
   "metadata": {},
   "source": [
    "## 1.2 Simple transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282152be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    Transforms the given DataFrame by extracting and formatting specific columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    round_num = df['SCHEDULE'].apply(lambda x: x.split('|')[0])  # Extract round number from 'SCHEDULE' column\n",
    "    date = df['SCHEDULE'].apply(lambda x: x.split('|')[1].split('-')[1])  # Extract date from 'SCHEDULE' column (Only the race date)\n",
    "\n",
    "    df.drop('SCHEDULE', axis=1, inplace=True)  # Drop unnecessary columns from the DataFrame\n",
    "\n",
    "    df['ROUND'] = round_num  # Add 'ROUND' column with extracted round numbers\n",
    "    df['DATE'] = pd.to_datetime(date)  # Convert extracted date to datetime format\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f193f",
   "metadata": {},
   "source": [
    "## 1.3 Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787d518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for race_id in all_race_ids:\n",
    "#    df = extract_data(race_id)\n",
    "#    transform_data(df)\n",
    "#    df.to_csv(f'DATA/id/{race_id}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c15625",
   "metadata": {},
   "source": [
    "# 2- TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8577bf0",
   "metadata": {},
   "source": [
    "## 2.1- Agroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de393059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data(race):\n",
    "    \"\"\"\n",
    "    Reduce the data in the 'race' DataFrame by replacing values in the 'TYPE' column and adding a new 'QUALI TYPE' column.\n",
    "\n",
    "    Args:\n",
    "        race (pandas.DataFrame): DataFrame containing the race data.\n",
    "\n",
    "    Returns:\n",
    "        None. The 'race' DataFrame is modified in place.\n",
    "    \"\"\"\n",
    "    def replace_ab(type):\n",
    "        if type == 'Qualifying Group B':\n",
    "            return 'Group B'\n",
    "        if type == 'Qualifying Group A':\n",
    "            return 'Group A'\n",
    "        else:\n",
    "            return 'Unique'\n",
    "    \n",
    "    # Replace values in 'TYPE' column using the 'replace_ab' function\n",
    "    race['QUALI TYPE'] = race['TYPE'].apply(replace_ab)\n",
    "    \n",
    "    # Replace specific values in 'TYPE' column\n",
    "    race['TYPE'].replace(['Qualifying Group B', 'Qualifying Group A'],'Qualifying Session',inplace=True)\n",
    "    race['TYPE'].replace('Sprint Race 1','Sprint Race',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a9a6d",
   "metadata": {},
   "source": [
    "## 2.2 Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2224b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(race):\n",
    "    \"\"\"\n",
    "    Extract specific data from the 'race' DataFrame based on the 'TYPE' column.\n",
    "\n",
    "    Args:\n",
    "        race (pandas.DataFrame): DataFrame containing the race data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data for each race type.\n",
    "    \"\"\"\n",
    "    \n",
    "    db_dict = {\n",
    "        'Sprint Race': None,\n",
    "        'Feature Race': None,\n",
    "        'Qualifying Session': None,\n",
    "        'Free Practice': None,\n",
    "        'Sprint Race 2': None\n",
    "    }\n",
    "    \n",
    "    db_drop = {\n",
    "        'Sprint Race': ['LAP SET ON','QUALI TYPE'],\n",
    "        'Feature Race': ['LAP SET ON','QUALI TYPE'],\n",
    "        'Qualifying Session': ['BEST','LAP'],\n",
    "        'Free Practice': ['BEST','LAP'],\n",
    "        'Sprint Race 2': ['LAP SET ON','QUALI TYPE'],\n",
    "    }\n",
    "\n",
    "    for key in db_dict:\n",
    "        # Extract data for each race type\n",
    "        db_dict[key] = race[race['TYPE'] == key]\n",
    "        \n",
    "        if key == 'Free Practice':\n",
    "            # Drop 'QUALI TYPE' column for 'Free Practice' race type (did not work with de dict)\n",
    "            db_dict[key] = db_dict[key].drop('QUALI TYPE',axis=1)\n",
    "        try:\n",
    "            # Drop additional columns specified in 'db_drop' dictionary (FIA change the metrics, so we need to handle exceptions)\n",
    "            db_dict[key] = db_dict[key].drop(db_drop[key],axis=1)\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "    return(db_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92829d",
   "metadata": {},
   "source": [
    "## 2.3 Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "path = r'C:\\Users\\alarc\\OneDrive - Instituto Tecnológico de Culiacán\\DEV\\Portfolio\\test-F2-database-ETL\\DATA\\id'\n",
    "\n",
    "all_racing = []\n",
    "for file in Path(path).glob('**/*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    all_racing.append(df)\n",
    "    \n",
    "len(all_racing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458488cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race in all_racing:\n",
    "    reduce_data(race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf91ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_db = {\n",
    "        'Sprint Race': pd.DataFrame(),\n",
    "        'Feature Race': pd.DataFrame(),\n",
    "        'Qualifying Session': pd.DataFrame(),\n",
    "        'Free Practice': pd.DataFrame(),\n",
    "        'Sprint Race 2': pd.DataFrame()\n",
    "    }\n",
    "\n",
    "for race in all_racing:\n",
    "    db = extract_data(race)\n",
    "    for key in db:\n",
    "        new_data_db[key] = pd.concat([new_data_db[key],db[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67d9ca",
   "metadata": {},
   "source": [
    "### 2.3.1 Save localy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in all_data.items():\n",
    "    filename = key.replace(' ','-')\n",
    "    value.to_csv(f'DATA/event/{filename}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034f7d9",
   "metadata": {},
   "source": [
    "# 3- LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41ea61",
   "metadata": {},
   "source": [
    "## 3.1 Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_files = ['Sprint-Race','Feature-Race','Qualifying-Session','Free-Practice','Sprint-Race-2']\n",
    "database_s3 = {}\n",
    "\n",
    "for file in db_files:\n",
    "    key = f'{file}.csv'\n",
    "    response = s3.get_object(Bucket='f2-events-db',Key=key)\n",
    "    status = response['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    if status == 200:\n",
    "        database_s3[file] = pd.read_csv(response['Body'])\n",
    "        print(f'Status - {status}: successful S3 get_object response. Key: {key}')\n",
    "    else:\n",
    "        print(f\"Status - {status}: unsuccessful S3 get_object response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ca170",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "bucket = 'f2-events-db'\n",
    "for key,value in database_s3.items():\n",
    "    new_data_key = key.replace('-',' ')\n",
    "    concatenated_db = pd.concat([value,new_data_db[new_data_key]])\n",
    "    \n",
    "    \n",
    "    csv_buffer = StringIO()\n",
    "    concatenated_db.to_csv(csv_buffer,index=False)\n",
    "    \n",
    "    client.put_object(\n",
    "        Body=csv_buffer.getvalue(),\n",
    "        Bucket=bucket,\n",
    "        Key=f'{key}.csv')\n",
    "    print(f'successful S3 put_object response. Key: {key}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323049ec",
   "metadata": {},
   "source": [
    "# 4- Update localy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a60fdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_race = '1057'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c2462ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = extract_data(new_race)\n",
    "reduce_data(new_data)\n",
    "new_data.to_csv(f'DATA/id/race_id_{new_race}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee3577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_files = ['Sprint-Race','Feature-Race','Qualifying-Session','Free-Practice','Sprint-Race-2']\n",
    "database_s3 = {}\n",
    "\n",
    "for file in db_files:\n",
    "    database_s3[file] = pd.read_csv(f'DATA/event/{file}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "191c51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_data(new_data)\n",
    "new_entry = format_data(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7091fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in database_s3.items():\n",
    "    entry_key = key.replace('-',' ')\n",
    "    updated_df = pd.concat([value,new_entry[entry_key]])\n",
    "    updated_df.to_csv(f'DATA/event/{key}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea09117",
   "metadata": {},
   "source": [
    "# 5 Concat all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2d9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for value in database_s3.values():\n",
    "    df = pd.concat([df,value],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948a3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('DATA/full_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a3b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
